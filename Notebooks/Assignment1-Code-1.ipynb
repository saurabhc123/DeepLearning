{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import random\n",
    "from math import exp\n",
    "import sklearn as skl\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization \n",
    "This is where we initialize the dataset and the hidden network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load Dataset\n",
    "inputFilenameWithPath = 'train_data.txt'\n",
    "inputData = np.loadtxt(inputFilenameWithPath, delimiter=\",\")\n",
    "n_inputs = len(inputData[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in inputData]))\n",
    "# Initialize a single layer neural network with n_neurons in the hidden layer\n",
    "def initialize_network(n_inputs, n_neurons , n_outputs):\n",
    "\tnetwork = list()\n",
    "\thidden_layer1 = [{'weights' :[random() for layer in range(n_inputs)]} for i in range(n_neurons)]\n",
    "\tnetwork.append(hidden_layer1)\n",
    "\thidden_layer2 = [{'weights' :[random() for layer in range(n_neurons)]} for i in range(n_neurons)]\n",
    "\tnetwork.append(hidden_layer2)\n",
    "\toutput_layer = [{'weights' :[random() for layer in range(n_neurons)]} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the input and weights initialization\n",
    "Print the layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The product summation and sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate neuron activation for an input\n",
    "def weights_input_product(weights, inputs):\n",
    "\tsummation = 0\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tsummation += weights[i] * inputs[i]\n",
    "\treturn summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\treturn 1.0 / (1.0 + exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Send the list of outputs for each layer\n",
    "def forward_propagate(network, inputData):\n",
    "    outputs = []\n",
    "    inputRecord = inputData\n",
    "    for layer in network: # Iterate over the layers\n",
    "        layer_output = []\n",
    "        i = 0\n",
    "        for neuron in layer: # Iterate for all neurons\n",
    "            summation = weights_input_product(neuron['weights'],inputRecord)\n",
    "            if i < len(network):\n",
    "                activation = sigmoid(summation)\n",
    "            else:\n",
    "                activation = softmax(summation)\n",
    "            neuron['output'] = activation\n",
    "            layer_output.append(activation)\n",
    "        outputs.append(layer_output)\n",
    "        inputRecord = layer_output\n",
    "    return layer_output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    sum = np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    return np.divide(np.exp(z),sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    "\n",
    "def transfer_softmax_derivative(signal):\n",
    "    return signal*(1-signal) + (1 - signal)*signal\n",
    "\n",
    "\n",
    "def transfer_softmax_derivative1(signal):\n",
    "    return np.multiply( signal, 1 - signal ) + sum(\n",
    "            # handle the off-diagonal values\n",
    "            - signal * np.roll( signal, i, axis = 1 )\n",
    "            for i in xrange(1, signal.shape[1] )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tif j != len(network)-1:\n",
    "\t\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])#Sigmoid derivative for all other layers\n",
    "\t\t\telse:\n",
    "\t\t\t\tneuron['delta'] = errors[j] * transfer_softmax_derivative(neuron['output'])#softmax derivative for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train via SGD\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs,n_iterations):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tn_examples = len(train)        \n",
    "\t\trandom_samples = train[np.random.choice(train.shape[0], n_iterations, replace=False), :];\n",
    "\t\tfor row in random_samples:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\t#print(\"Expected shape\",expected)\n",
    "\t\t\texpected[int(row[-1])] = 1\n",
    "\t\t\t#expected[1] = 1\n",
    "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "\t\t\tbackward_propagate_error(network, expected)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    "\t\tprint('Epoch=%d, Loss=%.3f' % (epoch, sum_error))\n",
    "    \n",
    "\tprint(\"\\n\\nFinal Weights\")\n",
    "\tfor layer in network:\n",
    "\t\tlayerWeights = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tlayerWeights.append(neuron['weights'])\n",
    "\t\tprint layerWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    sum = np.sum(np.exp(z))\n",
    "    return np.divide(np.exp(z),sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a network\n",
    "p =[ 0.0871086  , 0.91817548]\n",
    "\n",
    "def predict(network, row):\n",
    "\tp=outputs = forward_propagate(network, row)\n",
    "\t#print(outputs[0],outputs[1])\n",
    "\t#print(np.asarray(outputs).T)\n",
    "\treturn outputs.index(max(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, Loss=52.244\n",
      "Epoch=1, Loss=50.731\n",
      "Epoch=2, Loss=51.008\n",
      "Epoch=3, Loss=50.463\n",
      "Epoch=4, Loss=50.423\n",
      "Epoch=5, Loss=48.988\n",
      "Epoch=6, Loss=46.836\n",
      "Epoch=7, Loss=49.481\n",
      "Epoch=8, Loss=50.626\n",
      "Epoch=9, Loss=48.714\n",
      "Epoch=10, Loss=49.636\n",
      "Epoch=11, Loss=48.874\n",
      "Epoch=12, Loss=49.836\n",
      "Epoch=13, Loss=47.529\n",
      "Epoch=14, Loss=48.372\n",
      "Epoch=15, Loss=47.247\n",
      "Epoch=16, Loss=45.760\n",
      "Epoch=17, Loss=43.879\n",
      "Epoch=18, Loss=40.270\n",
      "Epoch=19, Loss=35.094\n",
      "Epoch=20, Loss=33.815\n",
      "Epoch=21, Loss=28.657\n",
      "Epoch=22, Loss=24.737\n",
      "Epoch=23, Loss=19.592\n",
      "Epoch=24, Loss=17.558\n",
      "Epoch=25, Loss=14.127\n",
      "Epoch=26, Loss=11.559\n",
      "Epoch=27, Loss=10.089\n",
      "Epoch=28, Loss=7.402\n",
      "Epoch=29, Loss=7.585\n",
      "Epoch=30, Loss=7.641\n",
      "Epoch=31, Loss=6.224\n",
      "Epoch=32, Loss=6.629\n",
      "Epoch=33, Loss=6.694\n",
      "Epoch=34, Loss=5.181\n",
      "Epoch=35, Loss=5.359\n",
      "Epoch=36, Loss=3.786\n",
      "Epoch=37, Loss=2.836\n",
      "Epoch=38, Loss=4.066\n",
      "Epoch=39, Loss=6.995\n",
      "Epoch=40, Loss=4.208\n",
      "Epoch=41, Loss=4.398\n",
      "Epoch=42, Loss=2.625\n",
      "Epoch=43, Loss=2.552\n",
      "Epoch=44, Loss=4.102\n",
      "Epoch=45, Loss=4.388\n",
      "Epoch=46, Loss=1.949\n",
      "Epoch=47, Loss=4.345\n",
      "Epoch=48, Loss=5.043\n",
      "Epoch=49, Loss=1.500\n",
      "\n",
      "\n",
      "Final Weights\n",
      "[[-3.3530569356698732, 33.222875089505912], [3.3317140556307199, -30.024974788493161], [-4.1061745326416572, 55.70728264547737]]\n",
      "[[2.4562800365391735, -1.5563607124325263, 1.1947410043909505], [-2.830735658247017, 2.9016511995144807, -3.925057778454153], [5.381715552785427, -5.782066092490587, 4.321910567029543]]\n",
      "[[-2.7455063735355316, 3.2847570701102615, -13.305267170516341], [2.8216653771120828, -3.391885904817052, 10.296333679983363]]\n",
      "\n",
      "Training Precision=0.98\n",
      "Training Recall=0.98\n",
      "Training F1 Score=0.98\n",
      "\n",
      "---------------------------- Testing the predictions -------------------------\n",
      "\n",
      "Test Precision=0.98\n",
      "Test Recall=0.98\n",
      "Test F1 Score=0.98\n"
     ]
    }
   ],
   "source": [
    "# Run the code now\n",
    "\n",
    "#Do the training first.\n",
    "seed(1)\n",
    "inputFilenameWithPath = 'train_data.txt'\n",
    "prediction_op = np.vectorize(lambda data: predict(network, data))\n",
    "inputData = np.loadtxt(inputFilenameWithPath, delimiter=\",\")\n",
    "n_inputs = len(inputData[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in inputData]))\n",
    "n_neurons = 3\n",
    "network = initialize_network(n_inputs, n_neurons, n_outputs)\n",
    "sgd_learning_rate = 0.1\n",
    "numberOfEpochs  = 50\n",
    "numberOfExamplesPerEpoch = 100\n",
    "train_network(network, inputData, sgd_learning_rate, numberOfEpochs, n_outputs,numberOfExamplesPerEpoch)\n",
    "predictions = []\n",
    "truth = inputData[:,2]\n",
    "for row in inputData:\n",
    "\tprediction = predict(network, row)\n",
    "\tpredictions.append(prediction)\n",
    "f1 = skl.metrics.f1_score(truth, predictions, average='micro')  \n",
    "precision = skl.metrics.precision_score(truth, predictions, average='micro')\n",
    "recall = skl.metrics.recall_score(truth, predictions, average='micro')\n",
    "print('\\nTraining Precision=%.2f' % (precision))\n",
    "print('Training Recall=%.2f' % (recall))\n",
    "print('Training F1 Score=%.2f' % (f1))\n",
    "    \n",
    "print('\\n---------------------------- Testing the predictions -------------------------')    \n",
    "# Test making predictions with the network\n",
    "inputFilenameWithPath = 'test_data.txt'\n",
    "dataset = np.loadtxt(inputFilenameWithPath, delimiter=\",\")\n",
    "\n",
    "#predictions = prediction_op(dataset[:,:2])\n",
    "truth = dataset[:,2]\n",
    "predictions = []\n",
    "for row in dataset:\n",
    "\tprediction = predict(network, row)\n",
    "\tpredictions.append(prediction)\n",
    "\t#print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "f1 = skl.metrics.f1_score(truth, predictions, average='micro')  \n",
    "precision = skl.metrics.precision_score(truth, predictions, average='micro')\n",
    "recall = skl.metrics.recall_score(truth, predictions, average='micro')\n",
    "print('\\nTest Precision=%.2f' % (precision))\n",
    "print('Test Recall=%.2f' % (recall))\n",
    "print('Test F1 Score=%.2f' % (f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
