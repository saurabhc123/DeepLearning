{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math as math\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "class Gridcell:\n",
    "    def __init__(self,value,neighbors, rowIndex, colIndex):\n",
    "        self.value = value\n",
    "        self.neighbors = neighbors\n",
    "        self.rowIndex = rowIndex\n",
    "        self.colIndex = colIndex\n",
    "        self.is_terminal = False\n",
    "\n",
    "    def set_terminal(self, terminal):\n",
    "        self.is_terminal = terminal\n",
    "\n",
    "    def set_neighbors(self, indices):\n",
    "        for i in indices:\n",
    "            self.neighbors.append(Neighbor(input[i], i, 1.0))\n",
    "\n",
    "class Neighbor:\n",
    "    def __init__(self, cell, index, probability):\n",
    "        self.cell = cell\n",
    "        self.probability = probability\n",
    "        self.index = index\n",
    "\n",
    "    def is_terminal(self):\n",
    "        if not self.index == 3 | self.index == 7:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = 3\n",
    "cols = 4\n",
    "input = []\n",
    "for i in range(rows):\n",
    "    col_values = []\n",
    "    for j in range(cols):\n",
    "        col_values.append(Gridcell(0,[],i,j))\n",
    "    input.append(col_values)\n",
    "\n",
    "\n",
    "input[0][3].value = 10\n",
    "input[1][3].value = -10\n",
    "input[0][3].set_terminal(True)\n",
    "input[1][3].set_terminal(True)\n",
    "input[2][1].set_terminal(True)\n",
    "input[2][1].value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 20\n",
    "number_of_states = 13\n",
    "max_reward = np.float16(10)\n",
    "number_of_actions = 5\n",
    "gamma = 0.9\n",
    "MAX_REWARD = 10.0\n",
    "q_states = np.zeros((number_of_states,number_of_actions),dtype=np.float16)\n",
    "terminal = np.zeros((number_of_states), dtype = np.bool)\n",
    "R = np.zeros((number_of_states,number_of_states),dtype=np.float16)\n",
    "Policy = ['']*number_of_states\n",
    "\n",
    "R[3][12] = 10.0\n",
    "R[7][12] = -10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_transition_reward(state, new_state):\n",
    "    return R[state][new_state]\n",
    "\n",
    "def get_state_transition_probability(state, new_state):\n",
    "    return 0.8\n",
    "\n",
    "def get_new_states(input_cell):\n",
    "    states = []\n",
    "    if(input_cell.is_terminal):\n",
    "        states.append((Gridcell(0,[],3,0),'exit'))\n",
    "        return states\n",
    "    #go left\n",
    "    if(input_cell.colIndex > 0):\n",
    "        states.append((input[input_cell.rowIndex][input_cell.colIndex - 1],'left'))\n",
    "    #go right\n",
    "    if(input_cell.colIndex < cols - 1):\n",
    "        states.append((input[input_cell.rowIndex][input_cell.colIndex + 1],'right'))\n",
    "    #go up\n",
    "    if(input_cell.rowIndex > 0):\n",
    "        states.append((input[input_cell.rowIndex - 1][input_cell.colIndex],'up'))\n",
    "    #go down\n",
    "    if(input_cell.rowIndex < rows - 1):\n",
    "        states.append((input[input_cell.rowIndex + 1][input_cell.colIndex],'down'))\n",
    "\n",
    "    return states\n",
    "\n",
    "def get_new_state(input_cell, action):\n",
    "    if action == 0: #go left\n",
    "        if(input_cell.colIndex > 0):\n",
    "            return input[input_cell.rowIndex][input_cell.colIndex - 1]\n",
    "    elif action == 1: #go right\n",
    "        if(input_cell.colIndex < cols - 1):\n",
    "            return input[input_cell.rowIndex][input_cell.colIndex + 1]\n",
    "    elif action == 2: #go up\n",
    "        if(input_cell.rowIndex > 0):\n",
    "            return input[input_cell.rowIndex - 1][input_cell.colIndex]\n",
    "    elif action == 3: #go down\n",
    "        if(input_cell.rowIndex < rows - 1):\n",
    "            return input[input_cell.rowIndex + 1][input_cell.colIndex]\n",
    "    elif action == 4: #exit\n",
    "        return None\n",
    "\n",
    "def get_state_index(input_cell):\n",
    "    m = cols * (input_cell.rowIndex) + input_cell.colIndex\n",
    "    return m;\n",
    "\n",
    "def get_unintended_state_values(remaining_probability, possible_states, intended_state_index,h, state, V):\n",
    "    comulative_value = 0.0\n",
    "    distributed_probability = remaining_probability/(len(possible_states) - 1)\n",
    "    for unwanted_state in possible_states:\n",
    "        unintended_state_index = get_state_index(unwanted_state[0])\n",
    "        if(unintended_state_index == intended_state_index):\n",
    "            continue\n",
    "        # new_state = unwanted_state[0]\n",
    "        # action = unwanted_state[1]\n",
    "        # new_state_index = get_state_index(new_state)\n",
    "        state_transition_probability = distributed_probability\n",
    "        state_transition_reward = get_state_transition_reward(state, unintended_state_index)\n",
    "        discounted_future_reward = gamma * V[h - 1][unintended_state_index]\n",
    "        value = state_transition_probability * (state_transition_reward +\n",
    "                                                        discounted_future_reward)\n",
    "        comulative_value += value\n",
    "    return comulative_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_value_iteration(input):\n",
    "    V = np.zeros((horizon,number_of_states), dtype=np.float16)\n",
    "    for h in range(0,1):\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                V[h][i*cols + j] = 0.0\n",
    "            input.append(col_values)\n",
    "    for h in range(1,horizon):\n",
    "        previous_sum = np.sum(V[h - 1])\n",
    "        for i in range(0,number_of_states-1):#For each state\n",
    "            state = i\n",
    "            if(terminal[state]):\n",
    "                V[h][state] = V[h-1][state]\n",
    "                continue\n",
    "            values_so_far = []\n",
    "            actions_so_far = []\n",
    "            m = i/(rows +1)\n",
    "            n = i%cols\n",
    "            #print m,n\n",
    "            possible_states = get_new_states(input[m][n])\n",
    "            for state_context in possible_states:# For each state transition\n",
    "                new_state = state_context[0]\n",
    "                action = state_context[1]\n",
    "                new_state_index = get_state_index(new_state)\n",
    "                state_transition_probability = get_state_transition_probability(state, new_state_index)\n",
    "                state_transition_reward = get_state_transition_reward(state, new_state_index)\n",
    "                discounted_future_reward = gamma * V[h - 1][new_state_index]\n",
    "                value = state_transition_probability * (state_transition_reward +\n",
    "                                                        discounted_future_reward)\n",
    "                self_state = (Gridcell(0,[],m,n),'stay')\n",
    "                other_possible_states = list(possible_states)\n",
    "                other_possible_states.append(self_state)\n",
    "                unintended_state_values = get_unintended_state_values((1-state_transition_probability),\n",
    "                                                                      other_possible_states,\n",
    "                                                                      new_state_index, h, state, V)\n",
    "                value += unintended_state_values\n",
    "                values_so_far.append(value)\n",
    "                actions_so_far.append(str(action))\n",
    "                if (action == 'exit'):\n",
    "                    terminal[state] = True\n",
    "            if(len(values_so_far) == 0):\n",
    "                V[h][state] = 0\n",
    "                max_action_index = 0\n",
    "                Policy[state] = 'None'\n",
    "            else:\n",
    "                V[h][state] = max(values_so_far)\n",
    "                max_action_index = values_so_far.index(max(values_so_far))\n",
    "                Policy[state] = actions_so_far[max_action_index]\n",
    "            if(V[h][state] >= MAX_REWARD):\n",
    "                terminal[state] = True\n",
    "        new_sum = np.sum(V[h])\n",
    "        print \"Policy - Iteration-{}:{}\".format(h,Policy[0:number_of_states -1])\n",
    "        print \"Policy Quality:{}\".format(np.sum(V[h]))\n",
    "        if(h > 5): #Let the first few iterations go through for convergence check.\n",
    "            if(new_sum - previous_sum) < 0.01:\n",
    "                print \"Converged at iteration:{}. Values:\".format(h)\n",
    "                print np.array(V[h][0:number_of_states -1]).reshape((3,4))\n",
    "                #print \"Optimal policy:{}\".format(Policy[0:number_of_states -1])\n",
    "                print \"\\n\\nOptimal policy:\\n{}\".format(np.array(Policy[0:number_of_states -1]).reshape((3,4)))\n",
    "                return\n",
    "        #print V[h][0:number_of_states -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy - Iteration-1:['right', 'left', 'left', 'exit', 'right', 'left', 'left', 'exit', 'right', 'exit', 'left', 'left']\n",
      "Policy Quality:0.0\n",
      "Policy - Iteration-2:['right', 'left', 'right', 'exit', 'right', 'left', 'left', 'exit', 'right', 'exit', 'left', 'left']\n",
      "Policy Quality:4.6796875\n",
      "Policy - Iteration-3:['right', 'right', 'right', 'exit', 'right', 'left', 'up', 'exit', 'right', 'exit', 'left', 'left']\n",
      "Policy Quality:13.140625\n",
      "Policy - Iteration-4:['right', 'right', 'right', 'exit', 'up', 'up', 'up', 'exit', 'right', 'exit', 'up', 'left']\n",
      "Policy Quality:23.390625\n",
      "Policy - Iteration-5:['right', 'right', 'right', 'exit', 'right', 'up', 'up', 'exit', 'right', 'exit', 'up', 'left']\n",
      "Policy Quality:30.890625\n",
      "Policy - Iteration-6:['right', 'right', 'right', 'exit', 'right', 'up', 'up', 'exit', 'up', 'exit', 'up', 'left']\n",
      "Policy Quality:36.28125\n",
      "Policy - Iteration-7:['right', 'right', 'right', 'exit', 'up', 'up', 'up', 'exit', 'up', 'exit', 'up', 'left']\n",
      "Policy Quality:39.0\n",
      "Policy - Iteration-8:['right', 'right', 'right', 'exit', 'up', 'up', 'up', 'exit', 'up', 'exit', 'up', 'left']\n",
      "Policy Quality:40.625\n",
      "Policy - Iteration-9:['right', 'right', 'right', 'exit', 'up', 'up', 'up', 'exit', 'up', 'exit', 'up', 'left']\n",
      "Policy Quality:41.46875\n",
      "Policy - Iteration-10:['right', 'right', 'right', 'exit', 'up', 'up', 'up', 'exit', 'up', 'exit', 'up', 'left']\n",
      "Policy Quality:41.9375\n",
      "Policy - Iteration-11:['right', 'right', 'right', 'exit', 'up', 'up', 'up', 'exit', 'up', 'exit', 'up', 'left']\n",
      "Policy Quality:42.1875\n",
      "Policy - Iteration-12:['right', 'right', 'right', 'exit', 'up', 'up', 'up', 'exit', 'up', 'exit', 'up', 'left']\n",
      "Policy Quality:42.3125\n",
      "Policy - Iteration-13:['right', 'right', 'right', 'exit', 'up', 'up', 'up', 'exit', 'up', 'exit', 'up', 'left']\n",
      "Policy Quality:42.375\n",
      "Policy - Iteration-14:['right', 'right', 'right', 'exit', 'up', 'up', 'up', 'exit', 'up', 'exit', 'up', 'left']\n",
      "Policy Quality:42.375\n",
      "Converged at iteration:14. Values:\n",
      "[[ 5.078125    5.8671875   6.83203125  8.        ]\n",
      " [ 4.421875    4.875       5.19921875 -8.        ]\n",
      " [ 3.49023438  0.          4.140625    2.484375  ]]\n",
      "\n",
      "\n",
      "Optimal policy:\n",
      "[['right' 'right' 'right' 'exit']\n",
      " ['up' 'up' 'up' 'exit']\n",
      " ['up' 'exit' 'up' 'left']]\n"
     ]
    }
   ],
   "source": [
    "perform_value_iteration(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions : I am assuming that the terminal states are also non-deterministic. The action at the terminal states can be either exit or stay at the same cell. Also, the null states always have a zero reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
