{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_filename = 'text_file.txt'\n",
    "\n",
    "\n",
    "def get_words_from_file(input_filename):\n",
    "    f = open(input_filename,\"r\")\n",
    "    buffer = f.read()\n",
    "    words = buffer.split(' ')\n",
    "    word_dict = {}\n",
    "    word_list = []\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        if(not word_dict.__contains__(word)):\n",
    "            word_dict[word] = i\n",
    "            word_list.append(word)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    return word_dict, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_n_words_random(word_list, n):\n",
    "    random_index = random.randrange(1, len(word_list) -  n - 1)\n",
    "    return np.array(range(random_index - 1,random_index + n - 1)), np.array(range(random_index + n - 1,random_index + n))\n",
    "\n",
    "def get_n_words(word_list, start_index, n):\n",
    "    random_index = start_index\n",
    "    return np.array(range(random_index - 1,random_index + n - 1)), np.array(range(random_index + n - 1,random_index + n))\n",
    "\n",
    "def get_training_data(word_list):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for i in range(len(word_list)/batch_size):\n",
    "        x, y = get_n_words(word_list, i*batch_size, n_steps)\n",
    "        train_x.append(x)\n",
    "        train_y.append(y)\n",
    "    return np.array(train_x), np.array(train_y)\n",
    "\n",
    "\n",
    "words_dict, words_list = get_words_from_file(input_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 1\n",
    "n_steps = 5\n",
    "n_neurons = 15\n",
    "n_outputs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_outputs])\n",
    "#cell = tf.contrib.rnn.BasicRNNCell(num_units = n_neurons, activation = tf.tanh)\n",
    "#cell = tf.contrib.rnn.BasicLSTMCell(n_neurons, forget_bias=1.0)\n",
    "#output_cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicRNNCell(num_units = n_neurons, activation = tf.tanh), output_size = n_outputs)\n",
    "#outputs , states = tf.nn.dynamic_rnn(output_cell, X, dtype= tf.float32)\n",
    "#logits = tf.layers.dense(states, n_outputs)\n",
    "#xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels= y , logits= logits)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_neurons,\n",
    "                                             forget_bias=1.0)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell,X, dtype=tf.float32)\n",
    "\n",
    "weights = {\n",
    "        'linear_layer': tf.Variable(tf.truncated_normal([n_neurons,\n",
    "                                                         n_outputs],\n",
    "                                                         mean=0,stddev=.01))\n",
    "}\n",
    "\n",
    "\n",
    "# Extract the last relevant output and use in a linear layer\n",
    "final_output = tf.matmul(states[1],\n",
    "                         weights[\"linear_layer\"])\n",
    "loss = tf.reduce_mean(tf.square(final_output - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.1, 0.9, 0.01)\n",
    "training_op = optimizer.minimize(loss)\n",
    "#correct = tf.nn.in_top_k(logits, y, 1)\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 1\n",
    "n_steps = 5\n",
    "n_neurons = 15\n",
    "n_outputs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_outputs])\n",
    "#cell = tf.contrib.rnn.BasicRNNCell(num_units = n_neurons, activation = tf.tanh)\n",
    "#cell = tf.contrib.rnn.BasicLSTMCell(n_neurons, forget_bias=1.0)\n",
    "#output_cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicRNNCell(num_units = n_neurons, activation = tf.tanh), output_size = n_outputs)\n",
    "#outputs , states = tf.nn.dynamic_rnn(output_cell, X, dtype= tf.float32)\n",
    "#logits = tf.layers.dense(states, n_outputs)\n",
    "#xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels= y , logits= logits)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_neurons,\n",
    "                                             forget_bias=1.0)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell,X, dtype=tf.float32)\n",
    "\n",
    "weights = {\n",
    "        'linear_layer': tf.Variable(tf.truncated_normal([n_neurons,\n",
    "                                                         n_outputs],\n",
    "                                                         mean=0,stddev=.01))\n",
    "}\n",
    "\n",
    "\n",
    "# Extract the last relevant output and use in a linear layer\n",
    "final_output = tf.matmul(states[1],\n",
    "                         weights[\"linear_layer\"])\n",
    "loss = tf.reduce_mean(tf.square(final_output - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.1, 0.9, 0.01)\n",
    "training_op = optimizer.minimize(loss)\n",
    "#correct = tf.nn.in_top_k(logits, y, 1)\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(sess, outputs):\n",
    "    random_index = 150\n",
    "    num_of_words_to_predict = 50\n",
    "    x_new = np.array(range(random_index,random_index + num_of_words_to_predict))\n",
    "    batch_x = x_new.reshape((-1,n_steps,n_inputs))\n",
    "    y_pred = sess.run(outputs, feed_dict={X: batch_x})\n",
    "    final_output = tf.matmul(states[1],\n",
    "                         weights[\"linear_layer\"])\n",
    "    print(final_output.eval(feed_dict={X: batch_x}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter 0, Minibatch Loss= 171707.78125\n",
      "[[ 0.38425231]\n",
      " [ 0.38448301]\n",
      " [ 0.38470754]\n",
      " [ 0.38492584]\n",
      " [ 0.38513777]\n",
      " [ 0.38534322]\n",
      " [ 0.38554227]\n",
      " [ 0.38573486]\n",
      " [ 0.385921  ]\n",
      " [ 0.38610074]]\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Iter 10, Minibatch Loss= 165181.265625\n",
      "[[ 9.73700142]\n",
      " [ 9.73705673]\n",
      " [ 9.73710442]\n",
      " [ 9.73714733]\n",
      " [ 9.73718357]\n",
      " [ 9.737216  ]\n",
      " [ 9.73724365]\n",
      " [ 9.73726845]\n",
      " [ 9.73729134]\n",
      " [ 9.73730946]]\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Iter 20, Minibatch Loss= 159966.6875\n",
      "[[ 17.38139915]\n",
      " [ 17.38142776]\n",
      " [ 17.38145447]\n",
      " [ 17.38147545]\n",
      " [ 17.38149643]\n",
      " [ 17.38151169]\n",
      " [ 17.38152504]\n",
      " [ 17.3815403 ]\n",
      " [ 17.38155174]\n",
      " [ 17.38156128]]\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Iter 30, Minibatch Loss= 154989.0625\n",
      "[[ 24.8321991 ]\n",
      " [ 24.83222198]\n",
      " [ 24.83224297]\n",
      " [ 24.83225632]\n",
      " [ 24.83227348]\n",
      " [ 24.83228683]\n",
      " [ 24.83229828]\n",
      " [ 24.83230972]\n",
      " [ 24.83231926]\n",
      " [ 24.83232498]]\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Iter 40, Minibatch Loss= 150175.09375\n",
      "[[ 32.19026184]\n",
      " [ 32.19028091]\n",
      " [ 32.19029617]\n",
      " [ 32.19031143]\n",
      " [ 32.19032669]\n",
      " [ 32.19033813]\n",
      " [ 32.19034958]\n",
      " [ 32.19035721]\n",
      " [ 32.19036865]\n",
      " [ 32.19037628]]\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Iter 50, Minibatch Loss= 145498.90625\n",
      "[[ 39.49038315]\n",
      " [ 39.49040222]\n",
      " [ 39.49041367]\n",
      " [ 39.49043274]\n",
      " [ 39.49044418]\n",
      " [ 39.49045563]\n",
      " [ 39.49046326]\n",
      " [ 39.4904747 ]\n",
      " [ 39.49048233]\n",
      " [ 39.49048996]]\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Iter 60, Minibatch Loss= 140947.0625\n",
      "[[ 46.75037003]\n",
      " [ 46.7503891 ]\n",
      " [ 46.75040436]\n",
      " [ 46.7504158 ]\n",
      " [ 46.75042725]\n",
      " [ 46.7504425 ]\n",
      " [ 46.75045013]\n",
      " [ 46.75046158]\n",
      " [ 46.75046539]\n",
      " [ 46.75047302]]\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Iter 70, Minibatch Loss= 136511.453125\n",
      "[[ 53.9809761 ]\n",
      " [ 53.98099136]\n",
      " [ 53.98101044]\n",
      " [ 53.9810257 ]\n",
      " [ 53.98103714]\n",
      " [ 53.98104477]\n",
      " [ 53.98105621]\n",
      " [ 53.98106766]\n",
      " [ 53.98107529]\n",
      " [ 53.98108292]]\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Iter 80, Minibatch Loss= 132186.71875\n",
      "[[ 61.1891861 ]\n",
      " [ 61.18921661]\n",
      " [ 61.18922806]\n",
      " [ 61.18924332]\n",
      " [ 61.18925858]\n",
      " [ 61.1892662 ]\n",
      " [ 61.18927383]\n",
      " [ 61.18928146]\n",
      " [ 61.18929291]\n",
      " [ 61.18930054]]\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Iter 90, Minibatch Loss= 127955.125\n",
      "[[ 68.37963867]\n",
      " [ 68.37986755]\n",
      " [ 68.38005829]\n",
      " [ 68.38019562]\n",
      " [ 68.38030243]\n",
      " [ 68.38039398]\n",
      " [ 68.38046265]\n",
      " [ 68.38051605]\n",
      " [ 68.38056183]\n",
      " [ 68.38059235]]\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Iter 100, Minibatch Loss= 123811.828125\n",
      "[[ 75.5796051 ]\n",
      " [ 75.57965851]\n",
      " [ 75.57968903]\n",
      " [ 75.57971954]\n",
      " [ 75.57974243]\n",
      " [ 75.57977295]\n",
      " [ 75.57978821]\n",
      " [ 75.57980347]\n",
      " [ 75.57981873]\n",
      " [ 75.57981873]]\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n",
      "Epoch: 108\n",
      "Epoch: 109\n",
      "Epoch: 110\n",
      "Iter 110, Minibatch Loss= 119764.78125\n",
      "[[ 82.76751709]\n",
      " [ 82.76757812]\n",
      " [ 82.76761627]\n",
      " [ 82.76764679]\n",
      " [ 82.76767731]\n",
      " [ 82.7677002 ]\n",
      " [ 82.76771545]\n",
      " [ 82.76773834]\n",
      " [ 82.76774597]\n",
      " [ 82.76776123]]\n",
      "Epoch: 111\n",
      "Epoch: 112\n",
      "Epoch: 113\n",
      "Epoch: 114\n",
      "Epoch: 115\n",
      "Epoch: 116\n",
      "Epoch: 117\n",
      "Epoch: 118\n",
      "Epoch: 119\n",
      "Epoch: 120\n",
      "Iter 120, Minibatch Loss= 115812.859375\n",
      "[[ 89.94523621]\n",
      " [ 89.9453125 ]\n",
      " [ 89.94535828]\n",
      " [ 89.94540405]\n",
      " [ 89.94543457]\n",
      " [ 89.94546509]\n",
      " [ 89.94548035]\n",
      " [ 89.94550323]\n",
      " [ 89.94552612]\n",
      " [ 89.94553375]]\n",
      "Epoch: 121\n",
      "Epoch: 122\n",
      "Epoch: 123\n",
      "Epoch: 124\n",
      "Epoch: 125\n",
      "Epoch: 126\n",
      "Epoch: 127\n",
      "Epoch: 128\n",
      "Epoch: 129\n",
      "Epoch: 130\n",
      "Iter 130, Minibatch Loss= 111955.046875\n",
      "[[ 97.1139679 ]\n",
      " [ 97.11401367]\n",
      " [ 97.11405182]\n",
      " [ 97.11407471]\n",
      " [ 97.11408997]\n",
      " [ 97.11411285]\n",
      " [ 97.11412048]\n",
      " [ 97.11413574]\n",
      " [ 97.11414337]\n",
      " [ 97.114151  ]]\n",
      "Epoch: 131\n",
      "Epoch: 132\n",
      "Epoch: 133\n",
      "Epoch: 134\n",
      "Epoch: 135\n",
      "Epoch: 136\n",
      "Epoch: 137\n",
      "Epoch: 138\n",
      "Epoch: 139\n",
      "Epoch: 140\n",
      "Iter 140, Minibatch Loss= 108191.03125\n",
      "[[ 104.27355957]\n",
      " [ 104.27360535]\n",
      " [ 104.27363586]\n",
      " [ 104.27366638]\n",
      " [ 104.27368927]\n",
      " [ 104.27370453]\n",
      " [ 104.27371979]\n",
      " [ 104.27373505]\n",
      " [ 104.27374268]\n",
      " [ 104.27375031]]\n",
      "Epoch: 141\n",
      "Epoch: 142\n",
      "Epoch: 143\n",
      "Epoch: 144\n",
      "Epoch: 145\n",
      "Epoch: 146\n",
      "Epoch: 147\n",
      "Epoch: 148\n",
      "Epoch: 149\n",
      "Epoch: 150\n",
      "Iter 150, Minibatch Loss= 104513.71875\n",
      "[[ 111.4270401 ]\n",
      " [ 111.42869568]\n",
      " [ 111.42949677]\n",
      " [ 111.42989349]\n",
      " [ 111.43009186]\n",
      " [ 111.4302063 ]\n",
      " [ 111.43026733]\n",
      " [ 111.43029022]\n",
      " [ 111.43031311]\n",
      " [ 111.43032837]]\n",
      "Epoch: 151\n",
      "Epoch: 152\n",
      "Epoch: 153\n",
      "Epoch: 154\n",
      "Epoch: 155\n",
      "Epoch: 156\n",
      "Epoch: 157\n",
      "Epoch: 158\n",
      "Epoch: 159\n",
      "Epoch: 160\n",
      "Iter 160, Minibatch Loss= 100918.429688\n",
      "[[ 118.58256531]\n",
      " [ 118.58470154]\n",
      " [ 118.58570099]\n",
      " [ 118.58620453]\n",
      " [ 118.58644867]\n",
      " [ 118.58657837]\n",
      " [ 118.58665466]\n",
      " [ 118.58670044]\n",
      " [ 118.58672333]\n",
      " [ 118.58673859]]\n",
      "Epoch: 161\n",
      "Epoch: 162\n",
      "Epoch: 163\n",
      "Epoch: 164\n",
      "Epoch: 165\n",
      "Epoch: 166\n",
      "Epoch: 167\n",
      "Epoch: 168\n",
      "Epoch: 169\n",
      "Epoch: 170\n",
      "Iter 170, Minibatch Loss= 97406.75\n",
      "[[ 125.73511505]\n",
      " [ 125.73757935]\n",
      " [ 125.7387085 ]\n",
      " [ 125.73923492]\n",
      " [ 125.73948669]\n",
      " [ 125.73961639]\n",
      " [ 125.73967743]\n",
      " [ 125.73970795]\n",
      " [ 125.73973083]\n",
      " [ 125.73974609]]\n",
      "Epoch: 171\n",
      "Epoch: 172\n",
      "Epoch: 173\n",
      "Epoch: 174\n",
      "Epoch: 175\n",
      "Epoch: 176\n",
      "Epoch: 177\n",
      "Epoch: 178\n",
      "Epoch: 179\n",
      "Epoch: 180\n",
      "Iter 180, Minibatch Loss= 93978.8359375\n",
      "[[ 132.88397217]\n",
      " [ 132.88641357]\n",
      " [ 132.88754272]\n",
      " [ 132.88807678]\n",
      " [ 132.88835144]\n",
      " [ 132.88851929]\n",
      " [ 132.88858032]\n",
      " [ 132.88864136]\n",
      " [ 132.88867188]\n",
      " [ 132.88870239]]\n",
      "Epoch: 181\n",
      "Epoch: 182\n",
      "Epoch: 183\n",
      "Epoch: 184\n",
      "Epoch: 185\n",
      "Epoch: 186\n",
      "Epoch: 187\n",
      "Epoch: 188\n",
      "Epoch: 189\n",
      "Epoch: 190\n",
      "Iter 190, Minibatch Loss= 90635.6484375\n",
      "[[ 140.02580261]\n",
      " [ 140.02856445]\n",
      " [ 140.02983093]\n",
      " [ 140.03045654]\n",
      " [ 140.03079224]\n",
      " [ 140.03097534]\n",
      " [ 140.03106689]\n",
      " [ 140.03115845]\n",
      " [ 140.03120422]\n",
      " [ 140.03123474]]\n",
      "Epoch: 191\n",
      "Epoch: 192\n",
      "Epoch: 193\n",
      "Epoch: 194\n",
      "Epoch: 195\n",
      "Epoch: 196\n",
      "Epoch: 197\n",
      "Epoch: 198\n",
      "Epoch: 199\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 200\n",
    "batch_size = 50\n",
    "batches = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_iterations):\n",
    "        print \"Epoch:\", epoch\n",
    "        for j in range(batches):  #((len(words_list) - n_steps)/batch_size):\n",
    "            batch_x , batch_y = get_training_data(words_list)\n",
    "            #print(batch_y.shape, batch_x.shape)\n",
    "            batch_y =  batch_y.reshape((-1,n_outputs))\n",
    "            batch_x = batch_x.reshape((-1,n_steps,n_inputs))\n",
    "            #print(batch_y.shape, batch_x.shape)\n",
    "            sess.run(training_op, feed_dict={X: batch_x, y: batch_y})\n",
    "        if epoch % 10 == 0:\n",
    "            mse = loss.eval(feed_dict={X: batch_x,y: batch_y})\n",
    "            print (\"Iter \" + str(epoch) + \", Minibatch Loss= \" + \\\n",
    "                    \"{}\".format(mse) )\n",
    "            test(sess, outputs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
