{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math as math\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "class Gridcell:\n",
    "    def __init__(self,value,neighbors, rowIndex, colIndex):\n",
    "        self.value = value\n",
    "        self.neighbors = neighbors\n",
    "        self.rowIndex = rowIndex\n",
    "        self.colIndex = colIndex\n",
    "        self.is_terminal = False\n",
    "\n",
    "    def set_terminal(self, terminal):\n",
    "        self.is_terminal = terminal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rows = 3\n",
    "cols = 4\n",
    "input = []\n",
    "for i in range(rows):\n",
    "    col_values = []\n",
    "    for j in range(cols):\n",
    "        col_values.append(Gridcell(0,[],i,j))\n",
    "    input.append(col_values)\n",
    "\n",
    "\n",
    "input[0][3].value = 10\n",
    "input[1][3].value = -10\n",
    "input[0][3].set_terminal(True)\n",
    "input[1][3].set_terminal(True)\n",
    "input[2][1].set_terminal(True)\n",
    "input[2][1].value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "horizon = 10\n",
    "number_of_states = 13\n",
    "max_reward = np.float16(10)\n",
    "number_of_actions = 5\n",
    "gamma = 0.9\n",
    "MAX_REWARD = 10.0\n",
    "q_states = np.zeros((number_of_states,number_of_actions),dtype=np.float16)\n",
    "terminal = np.zeros((number_of_states), dtype = np.bool)\n",
    "R = np.zeros((number_of_states,number_of_states),dtype=np.float16)\n",
    "Q = []\n",
    "\n",
    "\n",
    "R[3][12] = 10.0\n",
    "R[7][12] = -10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_state_transition_reward(state, new_state):\n",
    "    return R[state][new_state]\n",
    "\n",
    "def get_state_transition_probability(state, new_state):\n",
    "    return 1.0\n",
    "\n",
    "def get_new_states(input_cell):\n",
    "    states = []\n",
    "    if(input_cell.is_terminal):\n",
    "        states.append((Gridcell(0,[],3,0),'exit'))\n",
    "        return states\n",
    "    #go left\n",
    "    if(input_cell.colIndex > 0):\n",
    "        states.append((input[input_cell.rowIndex][input_cell.colIndex - 1],'left'))\n",
    "    #go right\n",
    "    if(input_cell.colIndex < cols - 1):\n",
    "        states.append((input[input_cell.rowIndex][input_cell.colIndex + 1],'right'))\n",
    "    #go up\n",
    "    if(input_cell.rowIndex > 0):\n",
    "        states.append((input[input_cell.rowIndex - 1][input_cell.colIndex],'up'))\n",
    "    #go down\n",
    "    if(input_cell.rowIndex < rows - 1):\n",
    "        states.append((input[input_cell.rowIndex + 1][input_cell.colIndex],'down'))\n",
    "\n",
    "    return states\n",
    "\n",
    "def get_new_state(input_cell, action):\n",
    "    if action == 0: #go left\n",
    "        if(input_cell.colIndex > 0):\n",
    "            return input[input_cell.rowIndex][input_cell.colIndex - 1]\n",
    "    elif action == 1: #go right\n",
    "        if(input_cell.colIndex < cols - 1):\n",
    "            return input[input_cell.rowIndex][input_cell.colIndex + 1]\n",
    "    elif action == 2: #go up\n",
    "        if(input_cell.rowIndex > 0):\n",
    "            return input[input_cell.rowIndex - 1][input_cell.colIndex]\n",
    "    elif action == 3: #go down\n",
    "        if(input_cell.rowIndex < rows - 1):\n",
    "            return input[input_cell.rowIndex + 1][input_cell.colIndex]\n",
    "    elif action == 4: #exit\n",
    "        return None\n",
    "\n",
    "def get_state_index(input_cell):\n",
    "    m = cols * (input_cell.rowIndex) + input_cell.colIndex\n",
    "    return m;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_value_iteration(input):\n",
    "    V = np.zeros((horizon,number_of_states), dtype=np.float16)\n",
    "    for h in range(0,1):\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                V[h][i*cols + j] = 0.0\n",
    "            input.append(col_values)\n",
    "\n",
    "    for h in range(1,horizon):\n",
    "        previous_sum = np.sum(V[h - 1])\n",
    "        for i in range(0,number_of_states-1):\n",
    "            state = i\n",
    "            if(terminal[state]):\n",
    "                V[h][state] = V[h-1][state]\n",
    "                continue\n",
    "            values_so_far = []\n",
    "            actions_so_far = []\n",
    "            m = i/(rows +1)\n",
    "            n = i%cols\n",
    "            #print m,n\n",
    "            for state_context in get_new_states(input[m][n]):\n",
    "                new_state = state_context[0]\n",
    "                action = state_context[1]\n",
    "                new_state_index = get_state_index(new_state)\n",
    "                state_transition_probability = get_state_transition_probability(state, new_state_index)\n",
    "                state_transition_reward = get_state_transition_reward(state, new_state_index)\n",
    "                discounted_future_reward = gamma * V[h-1][new_state_index]\n",
    "                value = state_transition_probability *(state_transition_reward +\n",
    "                                                       discounted_future_reward)\n",
    "                values_so_far.append(value)\n",
    "                actions_so_far.append(action)\n",
    "\n",
    "                if(action == 'exit'):\n",
    "                    terminal[state] = True\n",
    "            V[h][state] = max(values_so_far)\n",
    "            if(V[h][state] >= MAX_REWARD):\n",
    "                terminal[state] = True\n",
    "\n",
    "        new_sum = np.sum(V[h])\n",
    "        if(h > 1): #Let the first iteration go through for convergence check.\n",
    "            if(new_sum - previous_sum) < 0.01:\n",
    "                print \"Converged at iteration:{}\".format(h)\n",
    "                generate_q_values(V[h])\n",
    "                for q_values in Q:\n",
    "                    print q_values\n",
    "                #print Q\n",
    "                #print V[h][0:number_of_states -1]\n",
    "                break\n",
    "        #print V[h][0:number_of_states -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_q_values(V_final):\n",
    "    for i in range(0,number_of_states-1):\n",
    "        state = i\n",
    "        values_so_far = []\n",
    "        actions_so_far = []\n",
    "        m = i/(rows +1)\n",
    "        n = i%cols\n",
    "        #print m,n\n",
    "        for state_context in get_new_states(input[m][n]):\n",
    "            new_state = state_context[0]\n",
    "            action = state_context[1]\n",
    "            new_state_index = get_state_index(new_state)\n",
    "            state_transition_probability = get_state_transition_probability(state, new_state_index)\n",
    "            state_transition_reward = get_state_transition_reward(state, new_state_index)\n",
    "            discounted_future_reward = gamma * V_final[new_state_index]\n",
    "            if((discounted_future_reward == 0.0) &  (terminal[new_state_index] == True)):#If fallen into the NULL node\n",
    "                discounted_future_reward = gamma * V_final[state] #Optimality is from the same node itself\n",
    "            value = state_transition_probability *(state_transition_reward +\n",
    "                                                   discounted_future_reward)\n",
    "\n",
    "            values_so_far.append(value)\n",
    "            actions_so_far.append(action)\n",
    "\n",
    "            if(action == 'exit'):\n",
    "                terminal[state] = True\n",
    "        action_dictionary = {}\n",
    "        for action_index in range(len(actions_so_far)):\n",
    "            action_dictionary['State:'+ str(state) + ' Action:' + actions_so_far[action_index]] = values_so_far[action_index]\n",
    "        Q.append(action_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration:7\n",
      "{'State:0 Action:down': 5.90625, 'State:0 Action:right': 7.2914062500000005}\n",
      "{'State:1 Action:down': 6.5636718749999998, 'State:1 Action:left': 6.5636718749999998, 'State:1 Action:right': 8.0999999999999996}\n",
      "{'State:2 Action:left': 7.2914062500000005, 'State:2 Action:down': 7.2914062500000005, 'State:2 Action:right': 9.0}\n",
      "{'State:3 Action:exit': 10.0}\n",
      "{'State:4 Action:right': 6.5636718749999998, 'State:4 Action:up': 6.5636718749999998, 'State:4 Action:down': 5.3156249999999998}\n",
      "{'State:5 Action:left': 5.90625, 'State:5 Action:right': 7.2914062500000005, 'State:5 Action:up': 7.2914062500000005, 'State:5 Action:down': 6.5636718749999998}\n",
      "{'State:6 Action:down': 6.5636718749999998, 'State:6 Action:right': -9.0, 'State:6 Action:up': 8.0999999999999996, 'State:6 Action:left': 6.5636718749999998}\n",
      "{'State:7 Action:exit': -10.0}\n",
      "{'State:8 Action:right': 5.3156249999999998, 'State:8 Action:up': 5.90625}\n",
      "{'State:9 Action:exit': 0.0}\n",
      "{'State:10 Action:up': 7.2914062500000005, 'State:10 Action:left': 6.5636718749999998, 'State:10 Action:right': 5.90625}\n",
      "{'State:11 Action:up': -9.0, 'State:11 Action:left': 6.5636718749999998}\n"
     ]
    }
   ],
   "source": [
    "perform_value_iteration(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result shows the q-values. Each row is the state from 0-11, for each cell in the grid, with row major values. The actions that are we cannot perform for a cell are not shown. For example, for the state 3 (Cell (1,4)) cannot go anywhere except exiting. Just to clarify, the state of exit is another state that we used and this state is only available for the terminal nodes. We consider the null node as a terminal node with 0 reward. Also, I modeled the problem internally with 13 states, where the 13th state is the exit state. It made the whole code very much in line encompassing all states and actions, including the exit state and exit action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
