{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import random\n",
    "from math import exp\n",
    "import sklearn as skl\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization \n",
    "This is where we initialize the dataset and the hidden network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load Dataset\n",
    "inputFilenameWithPath = 'train_data.txt'\n",
    "inputData = np.loadtxt(inputFilenameWithPath, delimiter=\",\")\n",
    "n_inputs = len(inputData[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in inputData]))\n",
    "# Initialize a single layer neural network with n_neurons in the hidden layer\n",
    "def initialize_network1(n_inputs, n_neurons , n_outputs, dropout_prob):\n",
    "\tnetwork = list()\n",
    "\thidden_layer1 = [{'weights' :[random() for layer in range(n_inputs)]} for i in range(n_neurons)]\n",
    "\tnetwork.append(hidden_layer1)\n",
    "\thidden_layer2 = [{'weights' :[random() for layer in range(n_neurons)]} for i in range(n_neurons)]\n",
    "\tnetwork.append(hidden_layer2)\n",
    "\toutput_layer = [{'weights' :[random() for layer in range(n_neurons)]} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n",
    "\n",
    "def initialize_network(n_inputs, n_neurons , n_outputs, dropout_prob):\n",
    "\tnetwork = list()\n",
    "\thidden_layer1 = [{'weights' :generate_layer(n_inputs),'dropped': decision(dropout_prob)} for i in range(n_neurons)]\n",
    "\tnetwork.append(hidden_layer1)\n",
    "\thidden_layer2 = [{'weights' :generate_layer(n_neurons),'dropped': decision(dropout_prob)} for i in range(n_neurons)]\n",
    "\tnetwork.append(hidden_layer2)\n",
    "\toutput_layer = [{'weights' :generate_layer(n_neurons),'dropped': decision(dropout_prob)} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n",
    "\n",
    "def generate_layer(nInputs):\n",
    "    layer = [random() for layer in range(nInputs)]\n",
    "    return layer\n",
    "\n",
    "def decision(probability):\n",
    "    return random() < probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the input and weights initialization\n",
    "Print the layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Testing code\n",
    "network = initialize_network(n_inputs, n_neurons, n_outputs, 0.3)\n",
    "for layer in network:\n",
    "    for neuron in layer:\n",
    "        print(neuron['dropped'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The product summation and sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate neuron activation for an input\n",
    "def weights_input_product(weights, inputs):\n",
    "\tsummation = 0\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tsummation += weights[i] * inputs[i]\n",
    "\treturn summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\treturn 1.0 / (1.0 + exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Send the list of outputs for each layer\n",
    "def forward_propagate(network, inputData):\n",
    "    outputs = []\n",
    "    inputRecord = inputData\n",
    "    for layer in network: # Iterate over the layers\n",
    "        layer_output = []\n",
    "        i = 0\n",
    "        for neuron in layer: # Iterate for all neurons\n",
    "            summation = weights_input_product(neuron['weights'],inputRecord)\n",
    "            if i < len(network):\n",
    "                activation = sigmoid(summation)\n",
    "            else:\n",
    "                activation = softmax(summation)\n",
    "            neuron['output'] = activation\n",
    "            layer_output.append(activation)\n",
    "        outputs.append(layer_output)\n",
    "        inputRecord = layer_output\n",
    "    return layer_output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    sum = np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    return np.divide(np.exp(z),sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    "\n",
    "def transfer_softmax_derivative(signal):\n",
    "    return signal*(1-signal) + (1 - signal)*signal\n",
    "\n",
    "\n",
    "def transfer_softmax_derivative1(signal):\n",
    "    return np.multiply( signal, 1 - signal ) + sum(\n",
    "            # handle the off-diagonal values\n",
    "            - signal * np.roll( signal, i, axis = 1 )\n",
    "            for i in xrange(1, signal.shape[1] )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tif j != len(network)-1:\n",
    "\t\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])#Sigmoid derivative for all other layers\n",
    "\t\t\telse:\n",
    "\t\t\t\tneuron['delta'] = errors[j] * transfer_softmax_derivative(neuron['output'])#softmax derivative for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train via SGD\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs,n_iterations):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tn_examples = len(train)        \n",
    "\t\trandom_samples = train[np.random.choice(train.shape[0], n_iterations, replace=False), :];\n",
    "\t\tfor row in random_samples:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\t#print(\"Expected shape\",expected)\n",
    "\t\t\texpected[int(row[-1])] = 1\n",
    "\t\t\t#expected[1] = 1\n",
    "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "\t\t\tbackward_propagate_error(network, expected)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    "\t\tprint('Epoch=%d, Loss=%.3f' % (epoch, sum_error))\n",
    "    \n",
    "\tprint(\"\\n\\nFinal Weights\")\n",
    "\tfor layer in network:\n",
    "\t\tlayerWeights = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tlayerWeights.append(neuron['weights'])\n",
    "\t\tprint layerWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    sum = np.sum(np.exp(z))\n",
    "    return np.divide(np.exp(z),sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a prediction with a network\n",
    "p =[ 0.0871086  , 0.91817548]\n",
    "\n",
    "def predict(network, row):\n",
    "\tp=outputs = forward_propagate(network, row)\n",
    "\t#print(outputs[0],outputs[1])\n",
    "\t#print(np.asarray(outputs).T)\n",
    "\treturn outputs.index(max(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, Loss=53.728\n",
      "Epoch=1, Loss=50.391\n",
      "Epoch=2, Loss=50.808\n",
      "Epoch=3, Loss=48.347\n",
      "Epoch=4, Loss=46.592\n",
      "Epoch=5, Loss=45.901\n",
      "Epoch=6, Loss=42.349\n",
      "Epoch=7, Loss=38.601\n",
      "Epoch=8, Loss=35.427\n",
      "Epoch=9, Loss=29.943\n",
      "Epoch=10, Loss=24.601\n",
      "Epoch=11, Loss=20.064\n",
      "Epoch=12, Loss=16.706\n",
      "Epoch=13, Loss=12.734\n",
      "Epoch=14, Loss=12.104\n",
      "Epoch=15, Loss=9.215\n",
      "Epoch=16, Loss=9.062\n",
      "Epoch=17, Loss=7.734\n",
      "Epoch=18, Loss=7.402\n",
      "Epoch=19, Loss=6.593\n",
      "Epoch=20, Loss=5.176\n",
      "Epoch=21, Loss=4.763\n",
      "Epoch=22, Loss=6.874\n",
      "Epoch=23, Loss=4.940\n",
      "Epoch=24, Loss=4.536\n",
      "Epoch=25, Loss=6.833\n",
      "Epoch=26, Loss=4.002\n",
      "Epoch=27, Loss=5.330\n",
      "Epoch=28, Loss=3.817\n",
      "Epoch=29, Loss=3.874\n",
      "Epoch=30, Loss=2.915\n",
      "Epoch=31, Loss=2.171\n",
      "Epoch=32, Loss=4.077\n",
      "Epoch=33, Loss=3.778\n",
      "Epoch=34, Loss=4.828\n",
      "Epoch=35, Loss=4.249\n",
      "Epoch=36, Loss=3.191\n",
      "Epoch=37, Loss=3.025\n",
      "Epoch=38, Loss=3.342\n",
      "Epoch=39, Loss=2.284\n",
      "Epoch=40, Loss=1.718\n",
      "Epoch=41, Loss=4.474\n",
      "Epoch=42, Loss=1.966\n",
      "Epoch=43, Loss=6.067\n",
      "Epoch=44, Loss=3.315\n",
      "Epoch=45, Loss=3.324\n",
      "Epoch=46, Loss=5.297\n",
      "Epoch=47, Loss=5.801\n",
      "Epoch=48, Loss=2.710\n",
      "Epoch=49, Loss=5.211\n",
      "\n",
      "\n",
      "Final Weights\n",
      "[[-2.4879803510576162, 11.93971482305418], [2.297270251562594, -9.3210595394311344], [-2.8058824534711584, 16.837737662074193]]\n",
      "[[-2.425215860229101, 2.627551032998378, -2.4341025682167015], [2.7271451948239718, -2.594996249043327, 1.4117171987492079], [2.454286202985964, -2.017290922015984, 1.905368923483605]]\n",
      "[[3.2223554839930406, -3.2383427734568087, -3.0369058511280236], [-3.1980863285632113, 3.211987933365912, 0.20242481430019488]]\n",
      "\n",
      "Training Precision=0.98\n",
      "Training Recall=0.98\n",
      "Training F1 Score=0.98\n",
      "\n",
      "---------------------------- Testing the predictions -------------------------\n",
      "\n",
      "Test Precision=0.98\n",
      "Test Recall=0.98\n",
      "Test F1 Score=0.98\n"
     ]
    }
   ],
   "source": [
    "# Run the code now\n",
    "\n",
    "#Do the training first.\n",
    "seed(1)\n",
    "inputFilenameWithPath = 'train_data.txt'\n",
    "prediction_op = np.vectorize(lambda data: predict(network, data))\n",
    "inputData = np.loadtxt(inputFilenameWithPath, delimiter=\",\")\n",
    "dropout_prob = 0.3\n",
    "n_inputs = len(inputData[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in inputData]))\n",
    "n_neurons = 3\n",
    "network = initialize_network(n_inputs, n_neurons, n_outputs, dropout_prob)\n",
    "sgd_learning_rate = 0.1\n",
    "numberOfEpochs  = 50\n",
    "numberOfExamplesPerEpoch = 100\n",
    "train_network(network, inputData, sgd_learning_rate, numberOfEpochs, n_outputs,numberOfExamplesPerEpoch)\n",
    "predictions = []\n",
    "truth = inputData[:,2]\n",
    "for row in inputData:\n",
    "\tprediction = predict(network, row)\n",
    "\tpredictions.append(prediction)\n",
    "f1 = skl.metrics.f1_score(truth, predictions, average='micro')  \n",
    "precision = skl.metrics.precision_score(truth, predictions, average='micro')\n",
    "recall = skl.metrics.recall_score(truth, predictions, average='micro')\n",
    "print('\\nTraining Precision=%.2f' % (precision))\n",
    "print('Training Recall=%.2f' % (recall))\n",
    "print('Training F1 Score=%.2f' % (f1))\n",
    "    \n",
    "print('\\n---------------------------- Testing the predictions -------------------------')    \n",
    "# Test making predictions with the network\n",
    "inputFilenameWithPath = 'test_data.txt'\n",
    "dataset = np.loadtxt(inputFilenameWithPath, delimiter=\",\")\n",
    "\n",
    "#predictions = prediction_op(dataset[:,:2])\n",
    "truth = dataset[:,2]\n",
    "predictions = []\n",
    "for row in dataset:\n",
    "\tprediction = predict(network, row)\n",
    "\tpredictions.append(prediction)\n",
    "\t#print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "f1 = skl.metrics.f1_score(truth, predictions, average='micro')  \n",
    "precision = skl.metrics.precision_score(truth, predictions, average='micro')\n",
    "recall = skl.metrics.recall_score(truth, predictions, average='micro')\n",
    "print('\\nTest Precision=%.2f' % (precision))\n",
    "print('Test Recall=%.2f' % (recall))\n",
    "print('Test F1 Score=%.2f' % (f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
