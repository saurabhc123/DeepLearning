{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import random\n",
    "from random import randint\n",
    "from math import exp\n",
    "import sklearn as skl\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(n_inputs, n_neurons , n_outputs, dropout_prob):\n",
    "\tnetwork = list()\n",
    "\thidden_layer1 = [{'weights' :generate_layer(n_inputs),'dropped': decision(0)} for i in range(n_neurons)]\n",
    "\tnetwork.append(hidden_layer1)\n",
    "\thidden_layer2 = [{'weights' :generate_layer(n_neurons),'dropped': decision(dropout_prob)} for i in range(n_neurons)]\n",
    "\tnetwork.append(hidden_layer2)\n",
    "\toutput_layer = [{'weights' :generate_layer(n_neurons),'dropped': decision(0)} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n",
    "\n",
    "def generate_layer(nInputs):\n",
    "    layer = [random() for layer in range(nInputs)]\n",
    "    return layer\n",
    "\n",
    "def decision(probability):\n",
    "    return random() < probability\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def weights_input_product(weights, inputs):\n",
    "\tsummation = 0\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tsummation += weights[i] * inputs[i]\n",
    "\treturn summation\n",
    "\n",
    "def sigmoid(z):\n",
    "\treturn 1.0 / (1.0 + exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send the list of outputs for each layer\n",
    "def forward_propagate(network, inputData):\n",
    "    outputs = []\n",
    "    inputRecord = inputData\n",
    "    for layer in network: # Iterate over the layers\n",
    "        layer_output = []\n",
    "        i = 0\n",
    "        for neuron in layer: # Iterate for all neurons\n",
    "            if neuron['dropped']:\n",
    "                neuron['weights'] = np.zeros(len(neuron['weights']))\n",
    "            summation = weights_input_product(neuron['weights'],inputRecord)\n",
    "            if i < len(network):\n",
    "                activation = sigmoid(summation)\n",
    "            else:\n",
    "                activation = softmax(summation)\n",
    "            if not neuron['dropped']:\n",
    "                neuron['output'] = activation\n",
    "            else:\n",
    "                neuron['output'] = 0\n",
    "            layer_output.append(activation)\n",
    "        outputs.append(layer_output)\n",
    "        inputRecord = layer_output\n",
    "    return layer_output\n",
    "\n",
    "def softmax1(z):\n",
    "    sum = np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    return np.divide(np.exp(z),sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    "\n",
    "def transfer_softmax_derivative(signal):\n",
    "    return signal*(1-signal) + (1 - signal)*signal\n",
    "\n",
    "\n",
    "def transfer_softmax_derivative1(signal):\n",
    "    return np.multiply( signal, 1 - signal ) + sum(\n",
    "            # handle the off-diagonal values\n",
    "            - signal * np.roll( signal, i, axis = 1 )\n",
    "            for i in xrange(1, signal.shape[1] )\n",
    "        )\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                #print('For Layer:',j)\n",
    "                for neuron in network[i + 1]:\n",
    "                    if not neuron['dropped']:\n",
    "                        error += (neuron['weights'][j] * neuron['delta'])\n",
    "                    #print('Neuron dropped:', neuron['dropped'],'Error:',error)\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                if not neuron['dropped']:\n",
    "                    errors.append(expected[j] - neuron['output'])\n",
    "                else:\n",
    "                    errors.append(expected[j])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            if not neuron['dropped']:\n",
    "                if j != len(network) - 1:\n",
    "                    neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])  # Sigmoid derivative for all other layers\n",
    "                else:\n",
    "                    neuron['delta'] = errors[j] * transfer_softmax_derivative(neuron['output'])#softmax derivative for the output layer\n",
    "            else:\n",
    "                neuron['delta'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                #print('For Layer:',j)\n",
    "                for neuron in network[i + 1]:\n",
    "                    if not neuron['dropped']:\n",
    "                        error += (neuron['weights'][j] * neuron['delta'])\n",
    "                    #print('Neuron dropped:', neuron['dropped'],'Error:',error)\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                if not neuron['dropped']:\n",
    "                    errors.append(expected[j] - neuron['output'])\n",
    "                else:\n",
    "                    errors.append(expected[j])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            if not neuron['dropped']:\n",
    "                if j != len(network) - 1:\n",
    "                    neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])  # Sigmoid derivative for all other layers\n",
    "                else:\n",
    "                    neuron['delta'] = errors[j] * transfer_softmax_derivative(neuron['output'])#softmax derivative for the output layer\n",
    "            else:\n",
    "                neuron['delta'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                if not neuron['dropped']:\n",
    "                    neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            if not neuron['dropped']:\n",
    "                neuron['weights'][-1] += l_rate * neuron['delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train via SGD\n",
    "def train_network(networks, train, l_rate, n_epoch, n_outputs,n_iterations):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tn_examples = len(train)\n",
    "        for iteration in range(n_iterations):\n",
    "            random_samples = train[np.random.choice(train.shape[0], 100, replace=False), :]\n",
    "            networkToPick = randint(0, len(networks) - 1)\n",
    "            #print('Network Index Picked:',networkToPick)\n",
    "            random_network = networks[networkToPick]\n",
    "            for row in random_samples:\n",
    "                outputs = forward_propagate(random_network, row)\n",
    "                expected = [0 for i in range(n_outputs)]\n",
    "                #print(\"Expected shape\",expected)\n",
    "                expected[int(row[-1])] = 1\n",
    "                #expected[1] = 1\n",
    "                sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "                backward_propagate_error(random_network, expected)\n",
    "                update_weights(random_network, row, l_rate)\n",
    "                #print('Epoch=%d, Loss=%.3f' % (epoch, sum_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    sum = np.sum(np.exp(z))\n",
    "    return np.divide(np.exp(z),sum)\n",
    "\n",
    "def get_weighted_network(networks, dropout_prob):\n",
    "    network = list()\n",
    "    hidden_layer1 = [{'weights' :generate_weighted_weights(networks,0,dropout_prob,n_inputs, i),'dropped':False} for i in range(n_neurons)]\n",
    "    network.append(hidden_layer1)\n",
    "    hidden_layer2 = [{'weights' :generate_weighted_weights(networks,1,dropout_prob,n_neurons, i),'dropped':False}  for i in range(n_neurons)]\n",
    "    network.append(hidden_layer2)\n",
    "    output_layer = [{'weights' :generate_weighted_weights(networks,2,dropout_prob,n_neurons, i),'dropped':False} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "def generate_weighted_weights(networks, layerIndex, dropout_prob, rowCount, columnCount):\n",
    "    weights = np.zeros(rowCount)\n",
    "    for network in networks:\n",
    "        layer = network[layerIndex]\n",
    "        neuron = layer[columnCount]\n",
    "        val = [(1 - dropout_prob) * neuron['weights'][i]  for i in range(rowCount)]\n",
    "        weights += val\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, row):\n",
    "\toutputs = forward_propagate(network, row)\n",
    "\t#print(outputs[0],outputs[1])\n",
    "\t#print(np.asarray(outputs).T)\n",
    "\treturn outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "inputFilenameWithPath = 'train_data.txt'\n",
    "inputData = np.loadtxt(inputFilenameWithPath, delimiter=\",\")\n",
    "n_inputs = len(inputData[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in inputData]))\n",
    "n_neurons = 3\n",
    "dropout_prob = 0.3\n",
    "\n",
    "#Testing code\n",
    "networks = []\n",
    "network1 = initialize_network(n_inputs, n_neurons, n_outputs, dropout_prob)\n",
    "networks.append(network1)\n",
    "networks.append(initialize_network(n_inputs, n_neurons, n_outputs, dropout_prob))\n",
    "networks.append(initialize_network(n_inputs, n_neurons, n_outputs, dropout_prob))\n",
    "networks.append(initialize_network(n_inputs, n_neurons, n_outputs, dropout_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_learning_rate = 0.1\n",
    "numberOfEpochs  = 50\n",
    "numberOfExamplesPerEpoch = 600\n",
    "\n",
    "train_network(networks, inputData, sgd_learning_rate, numberOfEpochs, n_outputs,numberOfExamplesPerEpoch)\n",
    "\n",
    "weightedNetwork = get_weighted_network(networks, dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------- Testing the predictions -------------------------\n",
      "\n",
      "Test Precision=0.98\n",
      "Test Recall=0.98\n",
      "Test F1 Score=0.98\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------------------- Testing the predictions -------------------------')\n",
    "# Test making predictions with the network\n",
    "inputFilenameWithPath = 'test_data.txt'\n",
    "dataset = np.loadtxt(inputFilenameWithPath, delimiter=\",\")\n",
    "\n",
    "#predictions = prediction_op(dataset[:,:2])\n",
    "truth = dataset[:,2]\n",
    "predictions = []\n",
    "for row in dataset:\n",
    "\tprediction = predict(networks[0], row)\n",
    "\tpredictions.append(prediction)\n",
    "\t#print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "f1 = skl.metrics.f1_score(truth, predictions, average='micro')\n",
    "precision = skl.metrics.precision_score(truth, predictions, average='micro')\n",
    "recall = skl.metrics.recall_score(truth, predictions, average='micro')\n",
    "print('\\nTest Precision=%.2f' % (precision))\n",
    "print('Test Recall=%.2f' % (recall))\n",
    "print('Test F1 Score=%.2f' % (f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the original Neural Network implementation, these precision, recall and the F1 scores are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}